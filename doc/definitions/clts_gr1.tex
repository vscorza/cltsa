In the present section controller synthesis is applied to specifications where the environment behavior is defined by a CLTS instance and the property to be satisfied as an LTL formula which falls into the GR(1) category.


\begin{definition}\label{def:gr1_clts_control_problem} \emph{(GR(1) CLTS control problem)} 
	Let \controlProblemDef be a CLTS control problem, $\controlProblem$ is a GR(1) CLTS control problem if $\varphi$ satisfies: 
	\[\varphi = (\bigwedge_{i=1}^k\square \Diamond \gamma_i \implies \bigwedge_{j=1}^l\square \Diamond \psi_j)\]
	In the previous definition $\gamma_1, \ldots , \gamma_k$, $\psi_1, \ldots , \psi_l$ are propositional LTL formulas over atomic propositions that represent a set of assumptions over the environment and a set of guarantees the system should satisfy.
\end{definition}

Once the base for CLTS control problems is given, it is desirable to define a way to systematically answer whether\controlProblem is realizable.  
For the particular case where the goal $\varphi$ is restricted to GR(1) formulas we will follow what has been done with LTS based specifications in ~\cite{DBLP:phd/ethos/DIppolito13} after the application of small progress measures ~\cite{jurdzinski2000small} to Generalized Streett[1] Games from ~\cite{juvekar2006minimizing}. The general outline of the approach is this: a ranking function is defined w.r.t. each state in $S$ and liveness goal $\psi_j$ in $\varphi$, then a fixed point (lifting) algorithm is applied for each goal $\psi_j$ in a round-robin fashion, once stabilization is achieved the answer to the realizability question is given by observing whether all the rankings for the initial state are not $\infty$.
Since the GR(1) formula has the following structure:

\[\varphi = (\bigwedge_{i=1}^k\square \Diamond \gamma_i \implies \bigwedge_{j=1}^l\square \Diamond \psi_j)\]

The solution to the GR(1) control problem can be seen as a game where the system tries to satisfy $\varphi$ and the environment tries to satisfy its complement:

\[\neg\varphi = \bigwedge_{i=1}^k\square \Diamond \gamma_i \wedge \bigvee_{j=1}^l\Diamond \square \neg\psi_j\]

Which can be decomposed as a dis-junction $\neg\varphi = \bigvee_{j=1}^l \neg\varphi_j$ where:

\[\neg\varphi_j = \bigwedge_{i=1}^k\square \Diamond \gamma_i \wedge \Diamond \square \neg\psi_j\]

Here $\neg\varphi_j$ can be interpreted as the region where \emph{all assumptions $\gamma_1$ to $\gamma_k$ are satisfied infinitely often while there is a point after which the goal $\psi_j$ is never satisfied}. Because of this, the environment wins if and only if it can find a goal that can be always falsified after a certain point while satisfying all the assumptions infinitely often. This observation is used to define a notion of distance from a given state to the goal $\psi_j$ and an upper bound after which satisfaction is unachievable. The range $|j|$ of ranking $R_j$ is defined as the set of states that satisfy $\gamma_i$ while not satisfying $\psi_j$ w.r.t. to the assumption $\gamma_i$ that maximizes this number. The idea is that if the environment has the option to keep the play within a region where $\gamma_i$ is satisfied more than $|j|$ times while not satisfying $\psi_j$ once, then it wins $\neg\varphi_j$ which means that it also wins $\neg\varphi$.  Since the game is determined, the environment wins if and only if the system loses, thus being able to check this property answers the $\varphi$ realizability question.


\begin{definition}\label{def:gr1_range_j} \emph{(Range of $\psi_j$)} 
Let \controlProblemDef be a GR(1) CLTS control problem, the range of each system liveness proposition $\psi_j$ is defined as:
\[|j|=max_i\{|\gamma_i - \psi_j|\} = max_i \{ | \{ s | \gamma_i \in \valuations (s) \wedge  \psi_j \not\in \valuations (s) \} | \} \]
\end{definition}

In order to incrementally compute the distance from a given state $s$ to a goal $\psi_j$ we define the step-increment of the rank, first by giving semantic to $(r,i)+_j 1$ and then by deciding when to apply the increment w.r.t. to the valuations that hold at $s$ and whether or not the distance is within the rank $|j|$ ($incr_s^j((r,i))$).

\begin{definition}\label{def:gr1_rank_increment} \emph{(Rank increment)} 
	Let $(r,i)$ or $\infty$ be a rank for $\psi_j$ a liveness property, we define its increment $(r,i)+_j 1$ as follows:
\begin{center}
	\begin{tabular}{r l}
		$(r,i)+_j 1 = $ & $\begin{cases}
		(r,i+1) & i < k\\
		(r+1,i) & i = k \wedge r < |j|\\
		Â´\infty & r = |j|
		\end{cases}$\\
		$\infty + 1 = $ & $\infty$\\
	\end{tabular}
\end{center}
\end{definition}



\begin{definition}\label{def:gr1_rank_state_increment} \emph{(Rank increment w.r.t state)} 
	Let $(r,i)$ or $\infty$ be a rank for $\psi_j$ a liveness property, and \controlProblemDef a GR(1) CLTS control problem, we define the rank increment w.r.t. state $s$ ($s \in S$) as follows:
	\begin{center}
		\begin{tabular}{r l}
			$incr_s^j((r,i)) = $ & $\begin{cases}
			(0,1) & \psi_j \in \valuations(s)\\
			(r,i)+_j 1 & \psi_j \not\in \valuations(s) \wedge \gamma_i \in \valuations(s)\\			
			(r,i) & \text{otherwise}\\
			\infty & r = |j|
			\end{cases}$\\
			$incr_s^j(\infty) = $ & $\infty$\\
		\end{tabular}
	\end{center}
\end{definition}

The evaluation of ranking $R_j$ at state $s$ will be updated by way of backward propagation, picking at each step of the algorithm the successor that maximizes distance to the goal for the environment, while minimizing it for the system. Since the CLTS automaton allows for transitions having both controllable and non controllable actions, the decision can be thought as being split in two, first the environment chooses one candidate the set $\nonControlSet(s)$ ($\actionLabel_{\nonControlSet} \in \nonControlSet(s)$), which represent of possible projections over $\nonControlSet$ for the labels from transitions coming out of $s$, then the system chooses one transition conforming to it ($(s,\actionLabel, s') \in \Delta$ s.t. $\actionLabel_{\nonControlSet} = \actionLabel\downarrow_{\nonControlSet}$). The set $min_c^j(s)$ contains the optimal choices for the system w.r.t. to goal $\psi_j$ and all possible environmental choices $\actionLabel_{\nonControlSet} \in \nonControlSet(s)$. Then $max_R^j(s)$ is the best option for the environment against the choices in $min_c^j(s)$.  The value of $best^j(s)$ is defined by three cases, the first sets it to $\infty$ when the state has no outgoing edges (deadlock), the second case assigns the best successor but according to the next goal in round-robin fashion since the actual goal is satisfied at state $s$, otherwise it assigns the best successor according to $max_R^j(s)$.

\begin{definition}\label{def:gr1_rank_best} \emph{(Rank best successor)} 
	Let \controlProblemDef be a GR(1) CLTS control problem, and $R_j: S \mapsto ((\mathbb{N} \times \{1 \ldots n\}) \cup \{\infty \})$ the function mapping a state to its current $j$ ranking, we define the best successor ranking w.r.t. state $s$ and liveness property $\psi_j$, $best^j(s)$ as:
	\begin{center}
		\begin{tabular}{r l}
			$\nonControlSet(s)=$&$ \{\actionLabel_{\nonControlSet} | \exists \actionLabel \in 2^{|\Sigma|} \wedge s' \in S, \actionLabel_{\nonControlSet} = \actionLabel\downarrow_{\nonControlSet}  \wedge (s,\actionLabel, s') \in \Delta \} $\\
			$succ_{\actionLabel_{\nonControlSet}}(s)=$ & $ \{s' | \exists \actionLabel \in 2^{|\Sigma|} \wedge \actionLabel_{\nonControlSet} = \actionLabel\downarrow_{\nonControlSet}  \wedge (s,\actionLabel, s') \in \Delta \} $ \\
			$min_c^j(s)=$ & $ \{s' | \exists \actionLabel_{\nonControlSet} \in \nonControlSet(s) \wedge R_j(s') = min(R_j(succ_{\actionLabel_{\nonControlSet}}(s))) \} $ \\			
			$max_R^j(s)=$ & $max(R_j(min_c^j(s)))$ \\						
%			$succ(s,j) =$ & $min(\{ R_j(s')| \forall \actionLabel_{\nonControlSet}:(\exists \actionLabel,s':(\actionLabel \downarrow_{\nonControlSet} = \actionLabel_{\nonControlSet}, (s,\actionLabel,s')\in \Delta))  \})$ \\			
%			$succ(s,j) =$ & $min(\{ R_j(s')| \forall \actionLabel_{\nonControlSet}:(\exists \actionLabel,s':(\actionLabel \downarrow_{\nonControlSet} = \actionLabel_{\nonControlSet}, (s,\actionLabel,s')\in \Delta))  \})$ \\
			$best^j(s) = $ & $\begin{cases}
			\infty & \nexists \actionLabel, s' : (s,\actionLabel,s') \in \Delta\\
			max_R^{j \oplus 1}(s) & \psi_j \in \valuations(s)\\			
			max_R^{j}(s) & \psi_j \not\in \valuations(s)\\			
			\end{cases}$\\
		\end{tabular}
	\end{center}
\end{definition}

The ranking function $R_j$ is updated through $update^j(R_j,s)$ by increasing the value for $s$ only if the immediate increase of the successors ranking is higher than the current value (due to previous updates of the backwards propagation algorithm).

\begin{definition}\label{def:gr1_update} \emph{(Rank update)} 
	Let $(r,i)$ or $\infty$ be a rank for $\psi_j$ a liveness property, and \controlProblemDef a GR(1) CLTS control problem, we define the rank update w.r.t. state $s$ ($s \in S$) as follows:
	\begin{center}
		\begin{tabular}{r l}
			$update^j(R_j,s) =$ & $\{ \hat{r}_s^j(s') | s' \in S\}$\\
			$\hat{r}_s^j(s') = $ & $\begin{cases}
			R_j(s') & s \neq s'\\
			max(\{R_j(s),incr_s^j(best^j(s))\}) & \text{otherwise}\\
			\end{cases}$\\
		\end{tabular}
	\end{center}
\end{definition}

The lifting algorithm can now be presented, once it reaches a fixed point the ranking is said to be \emph{stabilized} thus answering the realizability question w.r.t. $\varphi$.

\begin{definition}\label{def:gr1_lift} \emph{(Rank lifting algorithm)} 
	Let \controlProblemDef be a GR(1) CLTS control problem and $\mathbf{R}:R_1 \ldots R_l$ its ranking system, we define the rank lifting algorithm as :
	\vspace{1em}
	\begin{center}
		\begin{tabular}{l}
			$\forall s \in S, j \in [1\ldots l]:R_j(s) \leftarrow (0,1);$\\
			\textbf{while}$(\exists s,j:(R_j(s) \neq update^j(R_j,s))):$\\
			$\qquad$ \textbf{if }$update^j(R_j,s) = \infty:$\\			
			$\qquad \qquad \forall j \in [1\ldots l]:R_j(s) \leftarrow \infty;$\\						
			$\qquad$ \textbf{else}$:$\\
			$\qquad \qquad R_j(s) \leftarrow update^j(R_j,s);$
		\end{tabular}
	\end{center}
	
\end{definition}

In order to show that the lifting algorithm solves the GR(1) CLTS control problem we define the notion of stability for a ranking and then introduce the theorem that proves the equivalence between stability and realizability.

\begin{definition}\label{def:gr1_stable_ranking} \emph{(Stable ranking)} 
	Let \controlProblemDef be a GR(1) CLTS control problem and $\mathbf{R}:R_1 \ldots R_l$ its ranking system, we say that ranking $R_j$ is stable w.r.t. state $s$ iff:
	\vspace{1em}
	\begin{center}
		\begin{tabular}{l}
			If $\psi_j \in \valuations(s)$ and $best^{j \oplus 1}(s) = \infty$ then $R_j(s) = \infty$ (\textbf{lose by single goal violation})\\
			If $\psi_j \in \valuations(s)$ and $best^{j \oplus 1}(s) \neq \infty$ then $R_j(s) = (0,1)$ (\textbf{target $j$ state})\\			
			If $\psi_j \not\in \valuations(s)$, $R_j(s) = (r,i)$ and $\gamma_i \in \valuations(s)$ then  $R_j(s) > best^j(s)$ (\textbf{necessary increment})\\						
			If $\psi_j \not\in \valuations(s)$, $R_j(s) = (r,i)$ and $\gamma_i \not\in \valuations(s)$ then  $R_j(s) \geq best^j(s)$ (\textbf{plausible increment})\\
		\end{tabular}
	\end{center}
We will say that a system $\mathbf{R}:R_1 \ldots R_l$ is stable if all of its components $R_1 \ldots R_l$ are stable.
\end{definition}

The motivation behind the definition of stability can be decompose on a case by case basis, the \textbf{lose by single goal violation} case ensures that if a state is losing for a single liveness goal, then it should be losing for all the rankings, since system liveness conditions should be satisfied in conjunction whenever the liveness assumptions hold in conjunction. The \textbf{target $j$ state} case forces the ranking to stay in its initial value for those non-losing states that satisfy the liveness goal. The \textbf{necessary increment} ensures that whenever the assumption $\gamma_i$ is satisfied while goal $\psi_j$ is not at state $s$, the distance to the next state satisfying $\psi_j$ must be necessary larger than that defined by the best successor ranking, while \textbf{plausible increment} defines that if neither the current assumption nor the goal are satisfied, the distance should be at least equal than that of the best successor ranking.

\begin{lemma}(\emph{Lifting algorithm fixed point achieves stability of}  $\mathbf{R}$)\label{theorem:gr1_lifting_stability}\\
	Let \controlProblemDef be a GR(1) CLTS control problem and $\mathbf{R}:R_1 \ldots R_l$ its ranking system, after applying the rank lifting algorithm, $\mathbf{R}$ is stable.
\end{lemma}

We will show that once the lifting algorithm reaches its fixed point the ranking system is stable.
At any given step within the algorithm either all rankings are set to $\infty$ for the current state $s$, or its value is effectively increased, following the definition of $update^j(s)$ and under the condition it is not equal to that $R_j(s)$, the value will be set to $incr_s^j(best^j(s))$.  Where $best^j(s)$ yields de best successor ranking according to the max-min criteria for both the environment and the system, and $incr_s^j(r)$ updates the ranking according to the conditions we will briefly review. These increments are applied until all $j$ rankings at any given state $s$ can not be increased further. The first stability condition, \textbf{lose by single goal violation}, is enforced by the first two lines of the \textbf{while loop}. The \textbf{target $j$ state} case holds by definition of $incr_s^j(r)$, since for each state $s$ s.t. $\psi_j \in \valuations(s)$ the ranking is fixed at $(0,1)$. The last two conditions (\textbf{necessary increment} and \textbf{plausible increment}) are achieved through the second case of the $update^j(s)$ function, where the assigned value is $max(\{R_j(s),incr_s^j(best^j(s))\})$. This assignment forces the value to be at least as big as the best max-min successor ranking by comparing it against the current value and forces it to be bigger when the conditions for the \textbf{necessary increment} are met, since they correspond to those for the second case of $incr_s^j(r)$, i.e.: $\psi_j \not\in \valuations(s)$ and $\gamma_i \in \valuations(s)$. 

\begin{theorem}(\emph{Stability solves realizability})\label{theorem:gr1_stability_realizability}\\
	Let \controlProblemDef be a GR(1) CLTS control problem and $\mathbf{R}:R_1 \ldots R_l$ its stable ranking system, then $R_1(s) \neq \infty$ if and only if there is a solution to \controlProblem.
	\normalsize
\end{theorem}

We will prove the previous theorem in two steps, first by showing that for each stable ranking system $\mathbf{R}$ we can build a CLTS controller $ctrl_{\mathbf{R}}(\mathbf{R})$ that is a solution to \controlProblem, and then that for each $C$ that solves \controlProblem, if we run the lifting algorithm over it, the stable ranking system yielded $\mathbf{R}$ satisfies $R_1(s) \neq \infty$.

For the first half of the proof we propose the following construction for the controller, where $ctrl_{\mathbf{R}}(\mathbf{R})= \langle S, \Sigma, s_0, \propositions, \valuations \rangle$:

\vspace{1em}
\begin{tabular}{ l c l }
	$S$ &$=$& $\{(s,\hat{m}) : s \in S_E, \hat{m} \in [0\ldots l-1] \}$\\
	$\Sigma$ &$=$&$\Sigma_E$\\	
	$s_0$&$=$&$(s_0^{E}, 0)$\\
	$\propositions$&$=$&$\propositions_E$\\	
	$\valuations$&$=$&$\pi_1 \circ \valuations_E$\\
	&&\\
	$\Delta$&$=$&$\{((s_i,j),\actionLabel,(s_j,j')): (s_i,\actionLabel,s_j) \in \Delta_E \wedge s_j \in max_{R}^j(s_i) \wedge$\\
	&&$(j' = j \iff \psi_j \not\in \valuations(s_j) \wedge j' = j \oplus 1 \iff \psi_j \in \valuations(s_j)))\}$\\
\end{tabular}
\vspace{1em}
\\

We can now show that every trace $\pi$ over $ctrl_{\mathbf{R}}(\mathbf{R})$ satisfies $\varphi$. Following the stable ranking system, since the initial state had a value different to $\infty$, each component, starting with $R_1$, that stands for $\psi_1$, forces the trace to be following a path where the distance to a state where the atomic goal is satisfied decrements monotonically, effectively decreasing it each time a state that satisfies $\gamma_i$ is reached. If there was a chance for the environment to pick a successor that forces the trace to falsify the goal $j$ indefinitely, this would have been captured by the definition of $best^j(s)$, first since it holds the worst case, i.e. maximum ranking value, w.r.t. to the minimum successors once the non controllable portion of the label is fixed, and second because if there is a chance to satisfy each assumption beyond $|j|$ the ranking will be set to $\infty$ and propagated back to the initial state. Satisfaction of all the components of $\bigwedge_j=1^l \psi_j$ is achieved by the round-robin concatenation of rankings once a state $s_j$ is reached such that $\psi_j$ is satisfied in $R_j(s_j)$, which is in turn followed by the construction of $\Delta$ since the second component of the target state is updated according to $\psi_j \in \valuations(s_j)$.

Next we proof that if $C$ is a solution to \controlProblem, running the lifting algorithm over it yields a stable ranking system $\mathbf{R}$ such that $R_1(s_0) \neq \infty$. PROVE